// Jest Snapshot v1, https://goo.gl/fbAQLP

exports[`getInputDefinition - if you pass a template and a variable, take value of the variable 1`] = `
[
  {
    "defaultValue": {
      "modelId": "test",
      "parameters": {},
    },
    "definition": {
      "anthropic/claude-2": {
        "contextLength": 8192,
        "description": "Claude: superior performance on tasks that require complex reasoning",
        "modelId": "anthropic/claude-2",
        "parameters": [
          {
            "defaultValue": 1,
            "description": "Amount of randomness injected into the response.",
            "id": "temperature",
            "max": 1,
            "min": 0,
            "name": "Temperature",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0.7,
            "description": "In nucleus sampling, we compute the cumulative distribution over all the options for each subsequent token in decreasing probability order and cut it off once it reaches a particular probability specified by top_p. You should either alter temperature or top_p, but not both.",
            "id": "top_p",
            "max": 1,
            "min": 0,
            "name": "Top P",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 5,
            "description": "Only sample from the top K options for each subsequent token.",
            "id": "top_k",
            "max": 100,
            "min": 0,
            "name": "Top K",
            "step": 1,
            "type": "number",
          },
        ],
        "settings": [
          {
            "description": "Your Open Router API key",
            "id": "open_router/api_key",
            "name": "Open Router API Key",
          },
        ],
      },
      "gpt-3.5-turbo-instruct": {
        "contextLength": 4096,
        "description": "Similar capabilities as text-davinci-003 but compatible with legacy Completions endpoint and not Chat Completions.",
        "modelId": "gpt-3.5-turbo-instruct",
        "parameters": [
          {
            "defaultValue": 0.4,
            "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.",
            "id": "temperature",
            "max": 2,
            "min": 0,
            "name": "Temperature",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 1024,
            "description": "The maximum number of tokens to generate in the completion. The total length of input tokens and generated tokens is limited by the model's context length.",
            "id": "max_tokens",
            "max": 4096,
            "min": 1,
            "name": "Max Tokens",
            "step": 20,
            "type": "number",
          },
          {
            "defaultValue": 1,
            "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.",
            "id": "top_p",
            "max": 1,
            "min": 0,
            "name": "Top P",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0,
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
            "id": "frequency_penalty",
            "max": 2,
            "min": -2,
            "name": "Frequency penalty",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0,
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
            "id": "presence_penalty",
            "max": 2,
            "min": -2,
            "name": "Presence penalty",
            "step": 0.1,
            "type": "number",
          },
        ],
        "settings": [
          {
            "description": "Your OpenAI API key",
            "id": "openai/api_key",
            "name": "API Key",
          },
        ],
        "streaming": true,
      },
      "gpt-4-1106-preview": {
        "contextLength": 4096,
        "description": "More capable than any GPT-3.5 model, able to do more complex tasks, and optimized for chat.",
        "modelId": "gpt-4-1106-preview",
        "parameters": [
          {
            "defaultValue": 0.4,
            "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.",
            "id": "temperature",
            "max": 2,
            "min": 0,
            "name": "Temperature",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 1024,
            "description": "The maximum number of tokens to generate in the completion. The total length of input tokens and generated tokens is limited by the model's context length.",
            "id": "max_tokens",
            "max": 4096,
            "min": 1,
            "name": "Max Tokens",
            "step": 20,
            "type": "number",
          },
          {
            "defaultValue": 1,
            "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.",
            "id": "top_p",
            "max": 1,
            "min": 0,
            "name": "Top P",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0,
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
            "id": "frequency_penalty",
            "max": 2,
            "min": -2,
            "name": "Frequency penalty",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0,
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
            "id": "presence_penalty",
            "max": 2,
            "min": -2,
            "name": "Presence penalty",
            "step": 0.1,
            "type": "number",
          },
        ],
        "settings": [
          {
            "description": "Your OpenAI API key",
            "id": "openai/api_key",
            "name": "API Key",
          },
        ],
        "streaming": true,
      },
      "meta-llama/llama-2-13b-chat": {
        "contextLength": 4096,
        "description": "Meta: Llama v2 13B Chat (beta)",
        "modelId": "meta-llama/llama-2-13b-chat",
        "parameters": [
          {
            "defaultValue": 0.4,
            "description": "",
            "id": "temperature",
            "max": 2,
            "min": 0,
            "name": "Temperature",
            "step": 0.1,
            "type": "number",
          },
        ],
        "settings": [
          {
            "description": "Your Open Router API key",
            "id": "open_router/api_key",
            "name": "Open Router API Key",
          },
        ],
      },
    },
    "description": "The model to use",
    "id": "model",
    "name": "AI Settings",
    "type": "model",
  },
  {
    "defaultValue": "summarize {{longText}}",
    "description": "The prompt to send to the LLM",
    "id": "prompt",
    "name": "Prompt",
    "type": "text",
  },
  {
    "defaultValue": "some long text",
    "description": "",
    "id": "longText",
    "name": "longText",
    "type": "text",
  },
]
`;

exports[`getInputDefinition - ignores non existing variables 1`] = `
[
  {
    "defaultValue": {
      "modelId": "test",
      "parameters": {},
    },
    "definition": {
      "anthropic/claude-2": {
        "contextLength": 8192,
        "description": "Claude: superior performance on tasks that require complex reasoning",
        "modelId": "anthropic/claude-2",
        "parameters": [
          {
            "defaultValue": 1,
            "description": "Amount of randomness injected into the response.",
            "id": "temperature",
            "max": 1,
            "min": 0,
            "name": "Temperature",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0.7,
            "description": "In nucleus sampling, we compute the cumulative distribution over all the options for each subsequent token in decreasing probability order and cut it off once it reaches a particular probability specified by top_p. You should either alter temperature or top_p, but not both.",
            "id": "top_p",
            "max": 1,
            "min": 0,
            "name": "Top P",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 5,
            "description": "Only sample from the top K options for each subsequent token.",
            "id": "top_k",
            "max": 100,
            "min": 0,
            "name": "Top K",
            "step": 1,
            "type": "number",
          },
        ],
        "settings": [
          {
            "description": "Your Open Router API key",
            "id": "open_router/api_key",
            "name": "Open Router API Key",
          },
        ],
      },
      "gpt-3.5-turbo-instruct": {
        "contextLength": 4096,
        "description": "Similar capabilities as text-davinci-003 but compatible with legacy Completions endpoint and not Chat Completions.",
        "modelId": "gpt-3.5-turbo-instruct",
        "parameters": [
          {
            "defaultValue": 0.4,
            "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.",
            "id": "temperature",
            "max": 2,
            "min": 0,
            "name": "Temperature",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 1024,
            "description": "The maximum number of tokens to generate in the completion. The total length of input tokens and generated tokens is limited by the model's context length.",
            "id": "max_tokens",
            "max": 4096,
            "min": 1,
            "name": "Max Tokens",
            "step": 20,
            "type": "number",
          },
          {
            "defaultValue": 1,
            "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.",
            "id": "top_p",
            "max": 1,
            "min": 0,
            "name": "Top P",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0,
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
            "id": "frequency_penalty",
            "max": 2,
            "min": -2,
            "name": "Frequency penalty",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0,
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
            "id": "presence_penalty",
            "max": 2,
            "min": -2,
            "name": "Presence penalty",
            "step": 0.1,
            "type": "number",
          },
        ],
        "settings": [
          {
            "description": "Your OpenAI API key",
            "id": "openai/api_key",
            "name": "API Key",
          },
        ],
        "streaming": true,
      },
      "gpt-4-1106-preview": {
        "contextLength": 4096,
        "description": "More capable than any GPT-3.5 model, able to do more complex tasks, and optimized for chat.",
        "modelId": "gpt-4-1106-preview",
        "parameters": [
          {
            "defaultValue": 0.4,
            "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.",
            "id": "temperature",
            "max": 2,
            "min": 0,
            "name": "Temperature",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 1024,
            "description": "The maximum number of tokens to generate in the completion. The total length of input tokens and generated tokens is limited by the model's context length.",
            "id": "max_tokens",
            "max": 4096,
            "min": 1,
            "name": "Max Tokens",
            "step": 20,
            "type": "number",
          },
          {
            "defaultValue": 1,
            "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.",
            "id": "top_p",
            "max": 1,
            "min": 0,
            "name": "Top P",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0,
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
            "id": "frequency_penalty",
            "max": 2,
            "min": -2,
            "name": "Frequency penalty",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0,
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
            "id": "presence_penalty",
            "max": 2,
            "min": -2,
            "name": "Presence penalty",
            "step": 0.1,
            "type": "number",
          },
        ],
        "settings": [
          {
            "description": "Your OpenAI API key",
            "id": "openai/api_key",
            "name": "API Key",
          },
        ],
        "streaming": true,
      },
      "meta-llama/llama-2-13b-chat": {
        "contextLength": 4096,
        "description": "Meta: Llama v2 13B Chat (beta)",
        "modelId": "meta-llama/llama-2-13b-chat",
        "parameters": [
          {
            "defaultValue": 0.4,
            "description": "",
            "id": "temperature",
            "max": 2,
            "min": 0,
            "name": "Temperature",
            "step": 0.1,
            "type": "number",
          },
        ],
        "settings": [
          {
            "description": "Your Open Router API key",
            "id": "open_router/api_key",
            "name": "Open Router API Key",
          },
        ],
      },
    },
    "description": "The model to use",
    "id": "model",
    "name": "AI Settings",
    "type": "model",
  },
  {
    "defaultValue": "summarize {{longText}}",
    "description": "The prompt to send to the LLM",
    "id": "prompt",
    "name": "Prompt",
    "type": "text",
  },
  {
    "defaultValue": "",
    "description": "",
    "id": "longText",
    "name": "longText",
    "type": "text",
  },
]
`;

exports[`getInputDefinition - no variables 1`] = `
[
  {
    "defaultValue": {
      "modelId": "test",
      "parameters": {},
    },
    "definition": {
      "anthropic/claude-2": {
        "contextLength": 8192,
        "description": "Claude: superior performance on tasks that require complex reasoning",
        "modelId": "anthropic/claude-2",
        "parameters": [
          {
            "defaultValue": 1,
            "description": "Amount of randomness injected into the response.",
            "id": "temperature",
            "max": 1,
            "min": 0,
            "name": "Temperature",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0.7,
            "description": "In nucleus sampling, we compute the cumulative distribution over all the options for each subsequent token in decreasing probability order and cut it off once it reaches a particular probability specified by top_p. You should either alter temperature or top_p, but not both.",
            "id": "top_p",
            "max": 1,
            "min": 0,
            "name": "Top P",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 5,
            "description": "Only sample from the top K options for each subsequent token.",
            "id": "top_k",
            "max": 100,
            "min": 0,
            "name": "Top K",
            "step": 1,
            "type": "number",
          },
        ],
        "settings": [
          {
            "description": "Your Open Router API key",
            "id": "open_router/api_key",
            "name": "Open Router API Key",
          },
        ],
      },
      "gpt-3.5-turbo-instruct": {
        "contextLength": 4096,
        "description": "Similar capabilities as text-davinci-003 but compatible with legacy Completions endpoint and not Chat Completions.",
        "modelId": "gpt-3.5-turbo-instruct",
        "parameters": [
          {
            "defaultValue": 0.4,
            "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.",
            "id": "temperature",
            "max": 2,
            "min": 0,
            "name": "Temperature",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 1024,
            "description": "The maximum number of tokens to generate in the completion. The total length of input tokens and generated tokens is limited by the model's context length.",
            "id": "max_tokens",
            "max": 4096,
            "min": 1,
            "name": "Max Tokens",
            "step": 20,
            "type": "number",
          },
          {
            "defaultValue": 1,
            "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.",
            "id": "top_p",
            "max": 1,
            "min": 0,
            "name": "Top P",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0,
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
            "id": "frequency_penalty",
            "max": 2,
            "min": -2,
            "name": "Frequency penalty",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0,
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
            "id": "presence_penalty",
            "max": 2,
            "min": -2,
            "name": "Presence penalty",
            "step": 0.1,
            "type": "number",
          },
        ],
        "settings": [
          {
            "description": "Your OpenAI API key",
            "id": "openai/api_key",
            "name": "API Key",
          },
        ],
        "streaming": true,
      },
      "gpt-4-1106-preview": {
        "contextLength": 4096,
        "description": "More capable than any GPT-3.5 model, able to do more complex tasks, and optimized for chat.",
        "modelId": "gpt-4-1106-preview",
        "parameters": [
          {
            "defaultValue": 0.4,
            "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.",
            "id": "temperature",
            "max": 2,
            "min": 0,
            "name": "Temperature",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 1024,
            "description": "The maximum number of tokens to generate in the completion. The total length of input tokens and generated tokens is limited by the model's context length.",
            "id": "max_tokens",
            "max": 4096,
            "min": 1,
            "name": "Max Tokens",
            "step": 20,
            "type": "number",
          },
          {
            "defaultValue": 1,
            "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.",
            "id": "top_p",
            "max": 1,
            "min": 0,
            "name": "Top P",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0,
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
            "id": "frequency_penalty",
            "max": 2,
            "min": -2,
            "name": "Frequency penalty",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0,
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
            "id": "presence_penalty",
            "max": 2,
            "min": -2,
            "name": "Presence penalty",
            "step": 0.1,
            "type": "number",
          },
        ],
        "settings": [
          {
            "description": "Your OpenAI API key",
            "id": "openai/api_key",
            "name": "API Key",
          },
        ],
        "streaming": true,
      },
      "meta-llama/llama-2-13b-chat": {
        "contextLength": 4096,
        "description": "Meta: Llama v2 13B Chat (beta)",
        "modelId": "meta-llama/llama-2-13b-chat",
        "parameters": [
          {
            "defaultValue": 0.4,
            "description": "",
            "id": "temperature",
            "max": 2,
            "min": 0,
            "name": "Temperature",
            "step": 0.1,
            "type": "number",
          },
        ],
        "settings": [
          {
            "description": "Your Open Router API key",
            "id": "open_router/api_key",
            "name": "Open Router API Key",
          },
        ],
      },
    },
    "description": "The model to use",
    "id": "model",
    "name": "AI Settings",
    "type": "model",
  },
  {
    "defaultValue": "summarize {{longText}}",
    "description": "The prompt to send to the LLM",
    "id": "prompt",
    "name": "Prompt",
    "type": "text",
  },
  {
    "defaultValue": "",
    "description": "",
    "id": "longText",
    "name": "longText",
    "type": "text",
  },
]
`;
