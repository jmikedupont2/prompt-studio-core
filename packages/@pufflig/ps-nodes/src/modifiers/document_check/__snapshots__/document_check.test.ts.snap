// Jest Snapshot v1, https://goo.gl/fbAQLP

exports[`documentCheck should extract variables correctly 1`] = `
[
  {
    "defaultValue": {
      "modelId": "test_model",
      "parameters": {},
    },
    "definition": {
      "anthropic/claude-2": {
        "contextLength": 8192,
        "description": "Claude: superior performance on tasks that require complex reasoning",
        "modelId": "anthropic/claude-2",
        "parameters": [
          {
            "defaultValue": 1,
            "description": "Amount of randomness injected into the response.",
            "id": "temperature",
            "max": 1,
            "min": 0,
            "name": "Temperature",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0.7,
            "description": "In nucleus sampling, we compute the cumulative distribution over all the options for each subsequent token in decreasing probability order and cut it off once it reaches a particular probability specified by top_p. You should either alter temperature or top_p, but not both.",
            "id": "top_p",
            "max": 1,
            "min": 0,
            "name": "Top P",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 5,
            "description": "Only sample from the top K options for each subsequent token.",
            "id": "top_k",
            "max": 100,
            "min": 0,
            "name": "Top K",
            "step": 1,
            "type": "number",
          },
        ],
        "settings": [
          {
            "description": "Your Open Router API key",
            "id": "open_router/api_key",
            "name": "Open Router API Key",
          },
        ],
      },
      "babbage-002": {
        "contextLength": 16384,
        "description": "Replacement for the GPT-3 ada and babbage base models.",
        "modelId": "babbage-002",
        "parameters": [
          {
            "defaultValue": 0.4,
            "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.",
            "id": "temperature",
            "max": 2,
            "min": 0,
            "name": "Temperature",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 1024,
            "description": "The maximum number of tokens to generate in the completion. The total length of input tokens and generated tokens is limited by the model's context length.",
            "id": "max_tokens",
            "max": 16384,
            "min": 1,
            "name": "Max Tokens",
            "step": 20,
            "type": "number",
          },
          {
            "defaultValue": 1,
            "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.",
            "id": "top_p",
            "max": 1,
            "min": 0,
            "name": "Top P",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0,
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
            "id": "frequency_penalty",
            "max": 2,
            "min": -2,
            "name": "Frequency penalty",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0,
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
            "id": "presence_penalty",
            "max": 2,
            "min": -2,
            "name": "Presence penalty",
            "step": 0.1,
            "type": "number",
          },
        ],
        "settings": [
          {
            "description": "Your OpenAI API key",
            "id": "openai/api_key",
            "name": "API Key",
          },
        ],
        "streaming": true,
      },
      "davinci-002": {
        "contextLength": 16384,
        "description": "Replacement for the GPT-3 curie and davinci base models.",
        "modelId": "davinci-002",
        "parameters": [
          {
            "defaultValue": 0.4,
            "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.",
            "id": "temperature",
            "max": 2,
            "min": 0,
            "name": "Temperature",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 1024,
            "description": "The maximum number of tokens to generate in the completion. The total length of input tokens and generated tokens is limited by the model's context length.",
            "id": "max_tokens",
            "max": 16384,
            "min": 1,
            "name": "Max Tokens",
            "step": 20,
            "type": "number",
          },
          {
            "defaultValue": 1,
            "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.",
            "id": "top_p",
            "max": 1,
            "min": 0,
            "name": "Top P",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0,
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
            "id": "frequency_penalty",
            "max": 2,
            "min": -2,
            "name": "Frequency penalty",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0,
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
            "id": "presence_penalty",
            "max": 2,
            "min": -2,
            "name": "Presence penalty",
            "step": 0.1,
            "type": "number",
          },
        ],
        "settings": [
          {
            "description": "Your OpenAI API key",
            "id": "openai/api_key",
            "name": "API Key",
          },
        ],
        "streaming": true,
      },
      "gpt-3.5-turbo": {
        "contextLength": 4096,
        "description": "Most capable GPT-3.5 model and optimized for chat at 1/10th the cost of text-davinci-003.",
        "modelId": "gpt-3.5-turbo",
        "parameters": [
          {
            "defaultValue": 0.4,
            "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.",
            "id": "temperature",
            "max": 2,
            "min": 0,
            "name": "Temperature",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 1024,
            "description": "The maximum number of tokens to generate in the completion. The total length of input tokens and generated tokens is limited by the model's context length.",
            "id": "max_tokens",
            "max": 4096,
            "min": 1,
            "name": "Max Tokens",
            "step": 20,
            "type": "number",
          },
          {
            "defaultValue": 1,
            "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.",
            "id": "top_p",
            "max": 1,
            "min": 0,
            "name": "Top P",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0,
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
            "id": "frequency_penalty",
            "max": 2,
            "min": -2,
            "name": "Frequency penalty",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0,
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
            "id": "presence_penalty",
            "max": 2,
            "min": -2,
            "name": "Presence penalty",
            "step": 0.1,
            "type": "number",
          },
        ],
        "settings": [
          {
            "description": "Your OpenAI API key",
            "id": "openai/api_key",
            "name": "API Key",
          },
        ],
        "streaming": true,
      },
      "gpt-3.5-turbo-16k": {
        "contextLength": 16385,
        "description": "Same capabilities as the standard gpt-3.5-turbo model but with 4 times the context.",
        "modelId": "gpt-3.5-turbo-16k",
        "parameters": [
          {
            "defaultValue": 0.4,
            "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.",
            "id": "temperature",
            "max": 2,
            "min": 0,
            "name": "Temperature",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 1024,
            "description": "The maximum number of tokens to generate in the completion. The total length of input tokens and generated tokens is limited by the model's context length.",
            "id": "max_tokens",
            "max": 16385,
            "min": 1,
            "name": "Max Tokens",
            "step": 20,
            "type": "number",
          },
          {
            "defaultValue": 1,
            "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.",
            "id": "top_p",
            "max": 1,
            "min": 0,
            "name": "Top P",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0,
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
            "id": "frequency_penalty",
            "max": 2,
            "min": -2,
            "name": "Frequency penalty",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0,
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
            "id": "presence_penalty",
            "max": 2,
            "min": -2,
            "name": "Presence penalty",
            "step": 0.1,
            "type": "number",
          },
        ],
        "settings": [
          {
            "description": "Your OpenAI API key",
            "id": "openai/api_key",
            "name": "API Key",
          },
        ],
        "streaming": true,
      },
      "gpt-3.5-turbo-instruct": {
        "contextLength": 4096,
        "description": "Similar capabilities as text-davinci-003 but compatible with legacy Completions endpoint and not Chat Completions.",
        "modelId": "gpt-3.5-turbo-instruct",
        "parameters": [
          {
            "defaultValue": 0.4,
            "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.",
            "id": "temperature",
            "max": 2,
            "min": 0,
            "name": "Temperature",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 1024,
            "description": "The maximum number of tokens to generate in the completion. The total length of input tokens and generated tokens is limited by the model's context length.",
            "id": "max_tokens",
            "max": 4096,
            "min": 1,
            "name": "Max Tokens",
            "step": 20,
            "type": "number",
          },
          {
            "defaultValue": 1,
            "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.",
            "id": "top_p",
            "max": 1,
            "min": 0,
            "name": "Top P",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0,
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
            "id": "frequency_penalty",
            "max": 2,
            "min": -2,
            "name": "Frequency penalty",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0,
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
            "id": "presence_penalty",
            "max": 2,
            "min": -2,
            "name": "Presence penalty",
            "step": 0.1,
            "type": "number",
          },
        ],
        "settings": [
          {
            "description": "Your OpenAI API key",
            "id": "openai/api_key",
            "name": "API Key",
          },
        ],
        "streaming": true,
      },
      "gpt-4": {
        "contextLength": 4096,
        "description": "More capable than any GPT-3.5 model, able to do more complex tasks, and optimized for chat.",
        "modelId": "gpt-4",
        "parameters": [
          {
            "defaultValue": 0.4,
            "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.",
            "id": "temperature",
            "max": 2,
            "min": 0,
            "name": "Temperature",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 1024,
            "description": "The maximum number of tokens to generate in the completion. The total length of input tokens and generated tokens is limited by the model's context length.",
            "id": "max_tokens",
            "max": 4096,
            "min": 1,
            "name": "Max Tokens",
            "step": 20,
            "type": "number",
          },
          {
            "defaultValue": 1,
            "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.",
            "id": "top_p",
            "max": 1,
            "min": 0,
            "name": "Top P",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0,
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
            "id": "frequency_penalty",
            "max": 2,
            "min": -2,
            "name": "Frequency penalty",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0,
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
            "id": "presence_penalty",
            "max": 2,
            "min": -2,
            "name": "Presence penalty",
            "step": 0.1,
            "type": "number",
          },
        ],
        "settings": [
          {
            "description": "Your OpenAI API key",
            "id": "openai/api_key",
            "name": "API Key",
          },
        ],
        "streaming": true,
      },
      "gpt-4-32k": {
        "contextLength": 32768,
        "description": "Same capabilities as the standard gpt-4 mode but with 4x the context length. ",
        "modelId": "gpt-4-32k",
        "parameters": [
          {
            "defaultValue": 0.4,
            "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.",
            "id": "temperature",
            "max": 2,
            "min": 0,
            "name": "Temperature",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 1024,
            "description": "The maximum number of tokens to generate in the completion. The total length of input tokens and generated tokens is limited by the model's context length.",
            "id": "max_tokens",
            "max": 32768,
            "min": 1,
            "name": "Max Tokens",
            "step": 20,
            "type": "number",
          },
          {
            "defaultValue": 1,
            "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.",
            "id": "top_p",
            "max": 1,
            "min": 0,
            "name": "Top P",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0,
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
            "id": "frequency_penalty",
            "max": 2,
            "min": -2,
            "name": "Frequency penalty",
            "step": 0.1,
            "type": "number",
          },
          {
            "defaultValue": 0,
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
            "id": "presence_penalty",
            "max": 2,
            "min": -2,
            "name": "Presence penalty",
            "step": 0.1,
            "type": "number",
          },
        ],
        "settings": [
          {
            "description": "Your OpenAI API key",
            "id": "openai/api_key",
            "name": "API Key",
          },
        ],
        "streaming": true,
      },
      "gpt2": {
        "contextLength": 512,
        "description": "",
        "modelId": "gpt2",
        "parameters": [
          {
            "defaultValue": 10,
            "description": "The temperature of the sampling operation. 1 means regular sampling, 0 means always take the highest score, 100.0 is getting closer to uniform probability.",
            "id": "temperature",
            "max": 100,
            "min": 0,
            "name": "Temperature",
            "step": 1,
            "type": "number",
          },
          {
            "defaultValue": 10,
            "description": "The amount of new tokens to be generated, this does not include the input length it is a estimate of the size of generated text you want. Each new tokens slows down the request, so look for balance between response times and length of text generated.",
            "id": "max_new_tokens",
            "max": 250,
            "min": 0,
            "name": "Max New Tokens",
            "step": 1,
            "type": "number",
          },
        ],
        "settings": [
          {
            "description": "Access tokens programmatically authenticate your identity to the Hugging Face Hub.",
            "id": "hf/access_token",
            "name": "User Access Tokens",
          },
        ],
        "streaming": false,
      },
      "gryphe/mythomax-l2-13b": {
        "contextLength": 4096,
        "description": "An improved, potentially even perfected variant of MythoMix.",
        "modelId": "gryphe/mythomax-l2-13b",
        "parameters": [
          {
            "defaultValue": 0.4,
            "description": "",
            "id": "temperature",
            "max": 2,
            "min": 0,
            "name": "Temperature",
            "step": 0.1,
            "type": "number",
          },
        ],
        "settings": [
          {
            "description": "Your Open Router API key",
            "id": "open_router/api_key",
            "name": "Open Router API Key",
          },
        ],
      },
      "meta-llama/llama-2-13b-chat": {
        "contextLength": 4096,
        "description": "Meta: Llama v2 13B Chat (beta)",
        "modelId": "meta-llama/llama-2-13b-chat",
        "parameters": [
          {
            "defaultValue": 0.4,
            "description": "",
            "id": "temperature",
            "max": 2,
            "min": 0,
            "name": "Temperature",
            "step": 0.1,
            "type": "number",
          },
        ],
        "settings": [
          {
            "description": "Your Open Router API key",
            "id": "open_router/api_key",
            "name": "Open Router API Key",
          },
        ],
      },
    },
    "description": "The model to use",
    "id": "model",
    "name": "Model",
    "type": "model",
  },
  {
    "defaultValue": "Hello, {{myVariable}}!",
    "description": "Prompt to check the document with",
    "id": "prompt",
    "name": "Prompt",
    "type": "text",
  },
  {
    "defaultValue": "test_table",
    "description": "The list, table or checklist to parse the document with.",
    "id": "table",
    "name": "Table",
    "type": "text",
  },
  {
    "defaultValue": "This is a test document.",
    "description": "Document to be processed",
    "id": "document",
    "name": "Document",
    "type": "text",
  },
  {
    "defaultValue": "myValue",
    "description": "",
    "id": "myVariable",
    "name": "myVariable",
    "type": "text",
  },
]
`;
